# CPU vs GPU Performance Comparison

Looking at the two training runs, here's a detailed comparison:

## üèÜ Overall Winner: **GPU** (22.5x faster training)

## ‚ö° Key Performance Metrics

### Training Performance
| Metric | CPU | GPU | GPU Speedup |
|--------|-----|-----|-------------|
| **Total Training Time** | 811.02s (13.52 min) | 35.97s (0.60 min) | **22.5x faster** |
| **Avg Time per Epoch** | 162.20s | 7.19s | **22.6x faster** |
| **Training Speed** | 370 samples/sec | 8,340 samples/sec | **22.5x faster** |

### Inference Performance
| Metric | CPU | GPU | Speedup |
|--------|-----|-----|---------|
| **Inference Speed** | 1,432 samples/sec | 4,464 samples/sec | **3.1x faster** |
| **Single Sample** | 91.29ms (¬±23.87) | 62.58ms (¬±7.88) | **1.5x faster** |
| **Evaluation Time** | 6.98s | 2.24s | **3.1x faster** |

### Setup & Preprocessing
| Metric | CPU | GPU | Difference |
|--------|-----|-----|------------|
| **Setup Time** | 11.61s | 11.51s | Similar |
| **Data Loading** | 0.47s | 2.24s | GPU slower (more overhead) |
| **Preprocessing** | 0.29s | 0.12s | GPU 2.4x faster |
| **Model Building** | 0.158s | 0.087s | GPU 1.8x faster |

## üìä Model Accuracy
- **CPU Test Accuracy**: 99.31%
- **GPU Test Accuracy**: 99.25%
- **Difference**: Essentially identical (within normal variance)

## üí° Key Insights

### Where GPU Excels
1. **Training**: Massive 22.5x speedup - GPUs are designed for parallel matrix operations
2. **Batch Processing**: GPU shines with larger batches during training
3. **Convolution Operations**: Conv2D layers benefit enormously from GPU parallelization

### Where CPU Holds Its Own
1. **Small Data Loading**: CPU was faster at initial data loading (0.47s vs 2.24s)
2. **Single Sample Inference**: Only 1.5x slower (still acceptable for production)
3. **Small Models**: For very simple models, the GPU overhead might not be worth it

### Interesting Observations
1. **GPU Variance**: GPU single inference has lower variance (7.88ms vs 23.87ms), suggesting more consistent performance
2. **First Epoch Anomaly**: GPU's first epoch took ~22s while subsequent epochs took ~3-4s (CUDA initialization overhead)
3. **Layer Inference**: CPU was actually faster for some individual layer operations (Conv2D: 14.36ms vs 181.12ms) due to GPU overhead for single operations

## üéØ Practical Recommendations

**Use GPU when:**
- Training deep neural networks (especially CNNs, LSTMs)
- Working with large datasets
- Iterating on model architectures frequently
- Time is critical

**CPU is fine when:**
- Deploying for single-sample inference
- Model is very small/simple
- GPU not available/cost is a concern
- Batch size is very small (< 8 samples)

## üí∞ Cost-Benefit Analysis
For this MNIST example:
- **Time Saved**: 775 seconds (12.9 minutes) per training run
- **Energy**: GPU uses more power but finishes much faster
- **Development**: GPU enables faster iteration cycles

**Bottom Line**: For any serious deep learning work, GPU acceleration is essentially mandatory. The 22x training speedup means you can experiment 22 times more in the same time period!
