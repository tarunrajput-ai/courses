# Multi-GPU Keras Training - Complete Guide
# Supports: Data Parallelism, Model Parallelism, and Mixed Strategies

"""
MULTI-GPU TRAINING STRATEGIES
==============================
1. Data Parallelism (Most Common): Same model on each GPU, split data
2. Model Parallelism: Split model layers across GPUs
3. Hybrid: Combine both strategies
"""

import time
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras import layers, models
from keras.datasets import mnist

# ============================================================================
# GPU DETECTION AND CONFIGURATION
# ============================================================================
print("="*80)
print("GPU DETECTION AND CONFIGURATION")
print("="*80)

# List all available GPUs
gpus = tf.config.list_physical_devices('GPU')
print(f"\nâœ… Number of GPUs Available: {len(gpus)}")

if len(gpus) > 0:
    for i, gpu in enumerate(gpus):
        print(f"   GPU {i}: {gpu.name}")
    
    # Enable memory growth (prevents allocating all GPU memory at once)
    for gpu in gpus:
        try:
            tf.config.experimental.set_memory_growth(gpu, True)
            print(f"âœ… Memory growth enabled for {gpu.name}")
        except RuntimeError as e:
            print(f"âš ï¸  Error setting memory growth: {e}")
    
    # Optional: Set memory limit per GPU (e.g., 4GB)
    # tf.config.set_logical_device_configuration(
    #     gpus[0],
    #     [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]
    # )
else:
    print("âš ï¸  No GPUs found! Training will use CPU.")

# ============================================================================
# STRATEGY 1: DATA PARALLELISM (Recommended for Most Cases)
# ============================================================================
print("\n" + "="*80)
print("STRATEGY 1: DATA PARALLELISM WITH MIRRORED STRATEGY")
print("="*80)
print("""
How it works:
- Same model replicated on each GPU
- Training data split across GPUs
- Gradients synchronized and averaged across GPUs
- Best for: Most training scenarios, scales well up to 8 GPUs
""")

# Create distribution strategy
if len(gpus) > 1:
    strategy = tf.distribute.MirroredStrategy()
    print(f"\nâœ… MirroredStrategy created")
    print(f"   Number of devices: {strategy.num_replicas_in_sync}")
elif len(gpus) == 1:
    strategy = tf.distribute.OneDeviceStrategy("/gpu:0")
    print(f"\nâœ… OneDeviceStrategy created (single GPU)")
else:
    strategy = tf.distribute.OneDeviceStrategy("/cpu:0")
    print(f"\nâš ï¸  CPU-only strategy (no GPU available)")

print(f"   Compute devices: {strategy.extended.worker_devices}")
print(f"   Parameter devices: {strategy.extended.parameter_devices}")

# ============================================================================
# LOAD AND PREPARE DATA
# ============================================================================
print("\n" + "="*80)
print("LOADING AND PREPROCESSING DATA")
print("="*80)

print("\nðŸ“¥ Loading MNIST dataset...")
load_start = time.time()
(x_train, y_train), (x_test, y_test) = mnist.load_data()
load_time = time.time() - load_start

print("ðŸ”„ Preprocessing data...")
preprocess_start = time.time()
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)
preprocess_time = time.time() - preprocess_start

print(f"âœ… Training data: {x_train.shape}")
print(f"âœ… Test data: {x_test.shape}")
print(f"â±ï¸  Data loading time: {load_time:.2f} seconds")
print(f"â±ï¸  Preprocessing time: {preprocess_time:.2f} seconds")

# ============================================================================
# CREATE DISTRIBUTED DATASET
# ============================================================================
print("\n" + "="*80)
print("CREATING DISTRIBUTED DATASET")
print("="*80)

# IMPORTANT: For multi-GPU training, use tf.data.Dataset
# This enables efficient data distribution across GPUs

BATCH_SIZE_PER_REPLICA = 128
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

print(f"\nðŸ“Š Batch Configuration:")
print(f"   Batch size per GPU: {BATCH_SIZE_PER_REPLICA}")
print(f"   Number of GPUs: {strategy.num_replicas_in_sync}")
print(f"   Global batch size: {GLOBAL_BATCH_SIZE}")
print(f"   Effective speedup: {strategy.num_replicas_in_sync}x")

# Create tf.data.Dataset for efficient pipeline
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.shuffle(10000).batch(GLOBAL_BATCH_SIZE)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_dataset = test_dataset.batch(GLOBAL_BATCH_SIZE)

# Distribute datasets across GPUs
train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)

print(f"âœ… Distributed datasets created")
print(f"   Train batches: ~{len(x_train) // GLOBAL_BATCH_SIZE}")
print(f"   Test batches: ~{len(x_test) // GLOBAL_BATCH_SIZE}")

# ============================================================================
# BUILD MODEL INSIDE STRATEGY SCOPE
# ============================================================================
print("\n" + "="*80)
print("BUILDING MODEL WITH DATA PARALLELISM")
print("="*80)

build_start = time.time()

# CRITICAL: Define model inside strategy scope for multi-GPU
with strategy.scope():
    model_data_parallel = models.Sequential([
        # Conv Block 1
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
        layers.BatchNormalization(),
        layers.Conv2D(32, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Conv Block 2
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),
        
        # Dense Block
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.BatchNormalization(),
        layers.Dropout(0.5),
        layers.Dense(10, activation='softmax')
    ], name='data_parallel_model')
    
    # Compile inside strategy scope
    model_data_parallel.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

build_time = time.time() - build_start

print("\nðŸ“Š Model Architecture:")
model_data_parallel.summary()
print(f"â±ï¸  Model building time: {build_time:.3f} seconds")

# Show where model parameters are stored
print(f"\nðŸ—ºï¸  Parameter Distribution:")
print(f"   Model parameters stored on: {strategy.extended.parameter_devices}")
print(f"   Each GPU gets a complete copy of the model")

# ============================================================================
# TRAIN WITH DATA PARALLELISM
# ============================================================================
print("\n" + "="*80)
print("TRAINING WITH MULTI-GPU DATA PARALLELISM")
print("="*80)

print("\nðŸš€ Training model (data distributed across GPUs)...\n")
train_start = time.time()

history = model_data_parallel.fit(
    train_dist_dataset,
    epochs=5,
    validation_data=test_dist_dataset,
    verbose=1
)

train_time = time.time() - train_start

print(f"\nâœ… Training completed!")
print(f"â±ï¸  Total training time: {train_time:.2f} seconds ({train_time/60:.2f} minutes)")
print(f"â±ï¸  Average time per epoch: {train_time/5:.2f} seconds")

total_samples = len(x_train) * 5
samples_per_second = total_samples / train_time
print(f"ðŸ“Š Training speed: {samples_per_second:.0f} samples/second")

# ============================================================================
# STRATEGY 2: MODEL PARALLELISM (For Very Large Models)
# ============================================================================
print("\n" + "="*80)
print("STRATEGY 2: MODEL PARALLELISM")
print("="*80)
print("""
How it works:
- Different layers on different GPUs
- Data flows sequentially through GPUs
- Best for: Very large models that don't fit on single GPU
- Limitation: No speedup for training time (sequential execution)
""")

if len(gpus) >= 2:
    print("\nðŸ—ï¸  Building model with layers split across GPUs...")
    
    with tf.device('/gpu:0'):
        inputs = layers.Input(shape=(28, 28, 1))
        x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.MaxPooling2D((2, 2))(x)
        x = layers.Dropout(0.25)(x)
    
    # Switch to GPU 1 for remaining layers
    with tf.device('/gpu:1'):
        x = layers.Conv2D(64, (3, 3), activation='relu')(x)
        x = layers.BatchNormalization()(x)
        x = layers.MaxPooling2D((2, 2))(x)
        x = layers.Dropout(0.25)(x)
        x = layers.Flatten()(x)
        x = layers.Dense(256, activation='relu')(x)
        x = layers.Dropout(0.5)(x)
        outputs = layers.Dense(10, activation='softmax')(x)
    
    model_model_parallel = models.Model(inputs=inputs, outputs=outputs, 
                                        name='model_parallel')
    model_model_parallel.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print("âœ… Model with layers split across 2 GPUs created")
    model_model_parallel.summary()
    
    print("\nðŸ—ºï¸  Layer Distribution:")
    for layer in model_model_parallel.layers:
        print(f"   {layer.name}: {layer.device if hasattr(layer, 'device') else 'default'}")
    
else:
    print("\nâš ï¸  Need at least 2 GPUs for model parallelism demo")

# ============================================================================
# STRATEGY 3: CUSTOM DISTRIBUTION (Advanced)
# ============================================================================
print("\n" + "="*80)
print("STRATEGY 3: CUSTOM TRAINING LOOP WITH MANUAL DISTRIBUTION")
print("="*80)
print("""
How it works:
- Full control over gradient computation and updates
- Can implement custom aggregation strategies
- Best for: Research, custom training logic
""")

if len(gpus) > 1:
    print("\nðŸ”§ Custom training loop example...")
    
    with strategy.scope():
        # Create model
        model_custom = models.Sequential([
            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
            layers.MaxPooling2D((2, 2)),
            layers.Flatten(),
            layers.Dense(128, activation='relu'),
            layers.Dense(10, activation='softmax')
        ])
        
        optimizer = keras.optimizers.Adam()
        loss_fn = keras.losses.CategoricalCrossentropy(
            reduction=tf.keras.losses.Reduction.NONE
        )
        
        def compute_loss(labels, predictions):
            per_example_loss = loss_fn(labels, predictions)
            return tf.nn.compute_average_loss(
                per_example_loss, 
                global_batch_size=GLOBAL_BATCH_SIZE
            )
        
        train_accuracy = keras.metrics.CategoricalAccuracy()
        
        @tf.function
        def train_step(inputs):
            images, labels = inputs
            
            with tf.GradientTape() as tape:
                predictions = model_custom(images, training=True)
                loss = compute_loss(labels, predictions)
            
            gradients = tape.gradient(loss, model_custom.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model_custom.trainable_variables))
            
            train_accuracy.update_state(labels, predictions)
            return loss
        
        @tf.function
        def distributed_train_step(dataset_inputs):
            per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
            return strategy.reduce(
                tf.distribute.ReduceOp.SUM, 
                per_replica_losses, 
                axis=None
            )
    
    print("âœ… Custom training loop defined")
    print("   This gives you full control over the training process")
    
    # Train for 1 epoch as demonstration
    print("\nðŸš€ Running 1 epoch with custom loop...")
    custom_start = time.time()
    
    for batch, dataset_inputs in enumerate(train_dist_dataset):
        if batch >= 100:  # Just 100 batches for demo
            break
        loss = distributed_train_step(dataset_inputs)
        if batch % 20 == 0:
            print(f"   Batch {batch}, Loss: {loss.numpy():.4f}, "
                  f"Accuracy: {train_accuracy.result().numpy():.4f}")
    
    custom_time = time.time() - custom_start
    print(f"\nâœ… Custom training demo completed in {custom_time:.2f}s")

# ============================================================================
# PERFORMANCE COMPARISON & MONITORING
# ============================================================================
print("\n" + "="*80)
print("PERFORMANCE ANALYSIS")
print("="*80)

print(f"\nðŸ“Š Data Parallelism Results:")
print(f"   GPUs used: {strategy.num_replicas_in_sync}")
print(f"   Global batch size: {GLOBAL_BATCH_SIZE}")
print(f"   Training time: {train_time:.2f}s")
print(f"   Samples/second: {samples_per_second:.0f}")
print(f"   Final accuracy: {history.history['accuracy'][-1]:.4f}")

# Calculate theoretical vs actual speedup
if strategy.num_replicas_in_sync > 1:
    print(f"\nâš¡ Scaling Efficiency:")
    print(f"   Number of GPUs: {strategy.num_replicas_in_sync}")
    print(f"   Theoretical speedup: {strategy.num_replicas_in_sync}x")
    print(f"   Note: Actual speedup depends on:")
    print(f"   â€¢ Communication overhead between GPUs")
    print(f"   â€¢ Batch size (larger = better GPU utilization)")
    print(f"   â€¢ Model size (larger = better parallelization)")
    print(f"   â€¢ Data pipeline efficiency")

# ============================================================================
# GPU MEMORY USAGE
# ============================================================================
print("\n" + "="*80)
print("GPU MEMORY USAGE")
print("="*80)

if len(gpus) > 0:
    print("\nðŸ’¾ Checking GPU memory...")
    
    # Get memory info (TensorFlow 2.x)
    for i, gpu in enumerate(gpus):
        # Note: Detailed memory info requires nvidia-smi or similar tools
        print(f"\n   GPU {i}:")
        print(f"   â€¢ Model replica stored: {'Yes' if i < strategy.num_replicas_in_sync else 'No'}")
        print(f"   â€¢ Estimated model size: ~1.27 MB (parameters)")
        print(f"   â€¢ Batch data per GPU: ~{BATCH_SIZE_PER_REPLICA * 28 * 28 * 4 / 1024:.2f} KB")
        print(f"   â€¢ Activations & gradients: ~50-100 MB")

# ============================================================================
# EVALUATION
# ============================================================================
print("\n" + "="*80)
print("MODEL EVALUATION")
print("="*80)

print("\nðŸ” Evaluating on test set...")
eval_start = time.time()
test_loss, test_acc = model_data_parallel.evaluate(test_dist_dataset, verbose=0)
eval_time = time.time() - eval_start

print(f"\nðŸ“Š FINAL RESULTS:")
print(f"âœ… Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"âœ… Test Loss: {test_loss:.4f}")
print(f"â±ï¸  Evaluation time: {eval_time:.2f} seconds")

# ============================================================================
# BEST PRACTICES FOR MULTI-GPU TRAINING
# ============================================================================
print("\n" + "="*80)
print("BEST PRACTICES FOR MULTI-GPU TRAINING")
print("="*80)

print("""
âœ… DO's:
1. Use MirroredStrategy for 1-8 GPUs (most common)
2. Use MultiWorkerMirroredStrategy for distributed training across machines
3. Increase batch size proportionally with number of GPUs
4. Use tf.data.Dataset.prefetch(AUTOTUNE) for efficient data loading
5. Enable mixed precision (FP16) for faster training: 
   tf.keras.mixed_precision.set_global_policy('mixed_float16')
6. Monitor GPU utilization (nvidia-smi, TensorBoard)
7. Build and compile model INSIDE strategy.scope()

âŒ DON'Ts:
1. Don't use too small batches (underutilizes GPUs)
2. Don't forget to scale learning rate with batch size
3. Don't use model parallelism unless model doesn't fit on single GPU
4. Don't mix strategy types without understanding implications
5. Don't forget about data loading bottlenecks

ðŸŽ¯ Optimal Configurations:
â€¢ 2 GPUs: batch_per_gpu=128, global_batch=256
â€¢ 4 GPUs: batch_per_gpu=64, global_batch=256
â€¢ 8 GPUs: batch_per_gpu=32, global_batch=256
â€¢ Adjust based on GPU memory and model size
""")

# ============================================================================
# SAVING MULTI-GPU MODELS
# ============================================================================
print("\n" + "="*80)
print("SAVING AND LOADING MULTI-GPU MODELS")
print("="*80)

print("""
ðŸ’¾ Saving:
model.save('my_model.keras')  # Saves complete model
model.save_weights('weights.h5')  # Saves only weights

ðŸ“‚ Loading:
# Load normally - Keras handles multi-GPU automatically
with strategy.scope():
    loaded_model = keras.models.load_model('my_model.keras')

Note: Model saved from multi-GPU can be loaded on single GPU and vice versa!
""")

# ============================================================================
# ADVANCED: MONITORING GPU UTILIZATION
# ============================================================================
print("\n" + "="*80)
print("MONITORING GPU UTILIZATION")
print("="*80)

print("""
Use these tools to monitor GPU performance:

1. nvidia-smi (Command line):
   watch -n 1 nvidia-smi

2. TensorBoard Profiler:
   tensorboard --logdir=./logs
   
3. Python GPU monitoring:
   import GPUtil
   GPUtil.showUtilization()

4. TensorFlow Profiler:
   tf.profiler.experimental.start('logdir')
   # ... training code ...
   tf.profiler.experimental.stop()

Key metrics to watch:
â€¢ GPU Utilization: Should be >80% during training
â€¢ Memory Usage: Should be stable, not growing
â€¢ SM Clock: Should be at max frequency
â€¢ Power Usage: Should be at or near TDP during training
""")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "="*80)
print("ðŸŽ‰ MULTI-GPU TRAINING COMPLETE!")
print("="*80)

print(f"""
ðŸ“Š Performance Summary:
â€¢ Number of GPUs: {strategy.num_replicas_in_sync}
â€¢ Training time: {train_time:.2f}s
â€¢ Throughput: {samples_per_second:.0f} samples/sec
â€¢ Final accuracy: {test_acc*100:.2f}%

ðŸš€ Scaling Strategy Used: Data Parallelism (MirroredStrategy)
â€¢ Each GPU: Complete model copy
â€¢ Data: Split across GPUs
â€¢ Gradients: Averaged across GPUs
â€¢ Communication: All-reduce algorithm

ðŸ’¡ Next Steps:
1. Try different batch sizes
2. Experiment with mixed precision training
3. Use gradient accumulation for larger effective batch sizes
4. Profile with TensorBoard
5. Try MultiWorkerMirroredStrategy for multi-machine training
""")

print("="*80)
print("Happy Multi-GPU Training! ðŸš€ðŸ’»")
print("="*80)
