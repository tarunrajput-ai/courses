# Keras Layers Complete Guide - Python Script for Google Colab
# Save this as a .py file or copy directly into Colab cells

"""
KERAS MODEL LAYERS - COMPLETE GUIDE WITH EXAMPLES
==================================================
Run this in Google Colab for GPU acceleration!
"""

# ============================================================================
# SETUP AND IMPORTS
# ============================================================================
print("Installing and importing libraries...")

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from keras import layers, models
from keras.datasets import mnist, fashion_mnist

print(f"âœ… TensorFlow version: {tf.__version__}")
print(f"âœ… Keras version: {keras.__version__}")
print(f"âœ… GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}")

# ============================================================================
# 1. DENSE LAYERS (FULLY CONNECTED)
# ============================================================================
print("\n" + "="*80)
print("1. DENSE LAYERS (FULLY CONNECTED)")
print("="*80)

model_dense = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

print("\nğŸ“Š Dense Layer Architecture:")
model_dense.summary()
print("\nFlow: Input(784) -> Dense(128) -> Dense(64) -> Dense(10)")

# ============================================================================
# 2. CONVOLUTIONAL LAYERS (Conv2D)
# ============================================================================
print("\n" + "="*80)
print("2. CONVOLUTIONAL LAYERS (Conv2D)")
print("="*80)

model_cnn = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

print("\nğŸ” CNN Architecture:")
model_cnn.summary()
print("\nğŸ’¡ Conv2D detects patterns like edges, shapes, and textures")

# ============================================================================
# 3. POOLING LAYERS
# ============================================================================
print("\n" + "="*80)
print("3. POOLING LAYERS")
print("="*80)

sample_input = np.random.rand(1, 8, 8, 1)
pooling_layer = layers.MaxPooling2D((2, 2))
pooled_output = pooling_layer(sample_input)

print(f"\nInput shape: {sample_input.shape}")
print(f"After MaxPooling(2,2): {pooled_output.shape}")
print(f"Size reduction: 75%")
print("\nâœ… Benefits: Reduces computation, provides translation invariance")

# ============================================================================
# 4. DROPOUT LAYERS
# ============================================================================
print("\n" + "="*80)
print("4. DROPOUT LAYERS")
print("="*80)

model_dropout = models.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dropout(0.5),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),
    layers.Dense(10, activation='softmax')
])

print("\nğŸ’Š Model with Dropout:")
model_dropout.summary()
print("\nğŸ’¡ Prevents overfitting by randomly dropping neurons during training")

# ============================================================================
# 5. BATCH NORMALIZATION
# ============================================================================
print("\n" + "="*80)
print("5. BATCH NORMALIZATION")
print("="*80)

model_bn = models.Sequential([
    layers.Dense(128, input_shape=(784,)),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dense(64),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.Dense(10, activation='softmax')
])

print("\nâš¡ Model with BatchNormalization:")
model_bn.summary()
print("\nâœ… Benefits: Faster training, higher learning rates possible")

# ============================================================================
# 6. RECURRENT LAYERS (LSTM)
# ============================================================================
print("\n" + "="*80)
print("6. RECURRENT LAYERS (LSTM)")
print("="*80)

model_lstm = models.Sequential([
    layers.LSTM(128, return_sequences=True, input_shape=(100, 10)),
    layers.LSTM(64),
    layers.Dense(10, activation='softmax')
])

print("\nğŸ”„ LSTM Architecture:")
model_lstm.summary()
print("\nğŸ“ Use cases: Time series, NLP, speech recognition, video analysis")

# ============================================================================
# 7. EMBEDDING LAYERS
# ============================================================================
print("\n" + "="*80)
print("7. EMBEDDING LAYERS")
print("="*80)

model_embed = models.Sequential([
    layers.Embedding(input_dim=10000, output_dim=128, input_length=100),
    layers.LSTM(64),
    layers.Dense(1, activation='sigmoid')
])

print("\nğŸ“– Model with Embedding:")
model_embed.summary()
print("\nğŸ’¡ Converts words (integers) to dense vectors for NLP tasks")

# ============================================================================
# 8. FLATTEN LAYER
# ============================================================================
print("\n" + "="*80)
print("8. FLATTEN LAYER")
print("="*80)

sample_conv = np.random.rand(1, 7, 7, 64)
flatten_layer = layers.Flatten()
flattened = flatten_layer(sample_conv)

print(f"\nBefore Flatten: {sample_conv.shape}")
print(f"After Flatten: {flattened.shape}")
print(f"Calculation: 7 Ã— 7 Ã— 64 = {7*7*64}")
print("\nâœ… Use: Transition from Conv layers to Dense layers")

# ============================================================================
# 9. COMPLETE MNIST EXAMPLE
# ============================================================================
print("\n" + "="*80)
print("9. COMPLETE MNIST EXAMPLE - TRAINING A REAL MODEL!")
print("="*80)

# Load data
print("\nğŸ“¥ Loading MNIST dataset...")
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Preprocess
x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255
x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

print(f"âœ… Training data: {x_train.shape}")
print(f"âœ… Test data: {x_test.shape}")

# Visualize samples
plt.figure(figsize=(10, 2))
for i in range(10):
    plt.subplot(1, 10, i+1)
    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')
    plt.title(f"{np.argmax(y_train[i])}")
    plt.axis('off')
plt.suptitle('Sample MNIST Digits', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# Build model
print("\nğŸ—ï¸ Building comprehensive model...")
model_complete = models.Sequential([
    # Conv Block 1
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.BatchNormalization(),
    layers.Conv2D(32, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Conv Block 2
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Dense Block
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

print("\nğŸ“Š Model Architecture:")
model_complete.summary()

# Compile
model_complete.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

print("\nâœ… Model compiled!")

# Train
print("\nğŸš€ Training model (this may take a few minutes)...\n")
history = model_complete.fit(
    x_train, y_train,
    batch_size=128,
    epochs=5,
    validation_split=0.1,
    verbose=1
)

# Evaluate
test_loss, test_acc = model_complete.evaluate(x_test, y_test, verbose=0)
print(f"\nğŸ“Š FINAL RESULTS:")
print(f"âœ… Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"âœ… Test Loss: {test_loss:.4f}")

# Plot training history
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(history.history['accuracy'], label='Training', linewidth=2)
ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)
ax1.set_xlabel('Epoch', fontsize=12)
ax1.set_ylabel('Accuracy', fontsize=12)
ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

ax2.plot(history.history['loss'], label='Training', linewidth=2)
ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)
ax2.set_xlabel('Epoch', fontsize=12)
ax2.set_ylabel('Loss', fontsize=12)
ax2.set_title('Model Loss', fontsize=14, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Predictions
predictions = model_complete.predict(x_test[:20], verbose=0)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test[:20], axis=1)

plt.figure(figsize=(15, 3))
for i in range(20):
    plt.subplot(2, 10, i+1)
    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')
    color = 'green' if predicted_classes[i] == true_classes[i] else 'red'
    plt.title(f"P:{predicted_classes[i]}\nT:{true_classes[i]}", 
              fontsize=10, color=color)
    plt.axis('off')

plt.suptitle('Predictions vs True Labels (Green=Correct, Red=Wrong)', 
             fontsize=12, fontweight='bold')
plt.tight_layout()
plt.show()

# ============================================================================
# 10. LAYER VISUALIZATION
# ============================================================================
print("\n" + "="*80)
print("10. LAYER VISUALIZATION")
print("="*80)

# Simple approach: manually pass data through layers
print("\nğŸ” Visualizing how data transforms through layers...")

sample_image = x_test[0:1]
print(f"\nOriginal input shape: {sample_image.shape}")

# Get the layers from our trained model
conv_layer_1 = model_complete.layers[0]  # First Conv2D
batch_norm_1 = model_complete.layers[1]  # BatchNorm

# Manually pass data through layers
conv1_output = conv_layer_1(sample_image)
print(f"After first Conv2D: {conv1_output.shape}")
print(f"  - Started with 1 channel, now have 32 feature maps")

# Apply batch normalization
bn1_output = batch_norm_1(conv1_output, training=False)
print(f"After BatchNormalization: {bn1_output.shape}")
print(f"  - Shape unchanged, but values normalized")

# Visualize the feature maps from first conv layer
print("\nğŸ“Š Visualizing first 10 (out of 32) feature maps...")

plt.figure(figsize=(15, 3))

# Original image
plt.subplot(1, 11, 1)
plt.imshow(sample_image[0, :, :, 0], cmap='gray')
plt.title('Original\nInput', fontsize=9, fontweight='bold')
plt.axis('off')

# Show first 10 feature maps
for i in range(10):
    plt.subplot(1, 11, i+2)
    feature_map = conv1_output[0, :, :, i].numpy()
    plt.imshow(feature_map, cmap='viridis')
    plt.title(f'Filter\n{i+1}', fontsize=9)
    plt.axis('off')
    plt.colorbar(fraction=0.046, pad=0.04)

plt.suptitle('Conv2D Feature Maps - Each Filter Detects Different Patterns', 
             fontsize=13, fontweight='bold', y=1.05)
plt.tight_layout()
plt.show()

print("\nğŸ’¡ Interpretation:")
print("  â€¢ Each feature map shows what a specific filter detected")
print("  â€¢ Brighter/warmer colors = stronger activation = feature detected")
print("  â€¢ Different filters detect edges, curves, corners, etc.")

# Show what happens after pooling
pooling_layer = model_complete.layers[4]  # MaxPooling2D
pooled_output = pooling_layer(bn1_output)
print(f"\nğŸ“‰ After MaxPooling2D(2,2): {pooled_output.shape}")
print(f"  - Reduced spatial dimensions by 75%: {conv1_output.shape[1:3]} â†’ {pooled_output.shape[1:3]}")
print(f"  - But kept all 32 feature maps")

# Compare original conv output vs pooled
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Before vs After MaxPooling - Filter 0', fontsize=14, fontweight='bold')

for i in range(5):
    # Before pooling
    axes[0, i].imshow(conv1_output[0, :, :, i].numpy(), cmap='viridis')
    axes[0, i].set_title(f'Before\n{conv1_output.shape[1]}x{conv1_output.shape[2]}', fontsize=10)
    axes[0, i].axis('off')
    
    # After pooling
    axes[1, i].imshow(pooled_output[0, :, :, i].numpy(), cmap='viridis')
    axes[1, i].set_title(f'After\n{pooled_output.shape[1]}x{pooled_output.shape[2]}', fontsize=10)
    axes[1, i].axis('off')

plt.tight_layout()
plt.show()

print("\nâœ… MaxPooling keeps the strongest activations while reducing size!")

# ============================================================================
# 11. BEST PRACTICES SUMMARY
# ============================================================================
print("\n" + "="*80)
print("11. BEST PRACTICES & KEY TAKEAWAYS")
print("="*80)

print("\nğŸ“‹ Common Architecture Patterns:")
print("\nğŸ–¼ï¸ Image Classification:")
print("  Conv2D -> BatchNorm -> MaxPooling -> Dropout")
print("  Repeat 2-3 times, then: Flatten -> Dense -> Dropout -> Output")

print("\nğŸ“ Text Classification:")
print("  Embedding -> LSTM/GRU -> Dropout -> Dense -> Output")

print("\nğŸ“Š Regression:")
print("  Dense -> BatchNorm -> Activation -> Dropout")
print("  Repeat, then: Dense(1, linear)")

print("\nğŸ¯ Activation Functions:")
print("  â€¢ Hidden layers: 'relu'")
print("  â€¢ Binary output: 'sigmoid'")
print("  â€¢ Multi-class: 'softmax'")
print("  â€¢ Regression: None or 'linear'")

print("\nğŸ›¡ï¸ Regularization:")
print("  â€¢ Dropout: 0.2-0.5")
print("  â€¢ BatchNormalization: After layer, before activation")
print("  â€¢ Early stopping: Monitor val_loss")

print("\nâš™ï¸ Training Tips:")
print("  â€¢ Optimizer: Adam (good default)")
print("  â€¢ Batch size: 32, 64, 128, 256")
print("  â€¢ Learning rate: Start with 0.001")
print("  â€¢ Use validation split: 0.1-0.2")

# ============================================================================
# SUMMARY
# ============================================================================
print("\n" + "="*80)
print("ğŸ‰ CONGRATULATIONS! YOU'VE COMPLETED THE KERAS LAYERS GUIDE!")
print("="*80)

print("\nâœ… What you learned:")
print("  1. Dense - Fully connected layers")
print("  2. Conv2D - Feature extraction from images")
print("  3. MaxPooling - Dimension reduction")
print("  4. Dropout - Prevent overfitting")
print("  5. BatchNormalization - Faster, stable training")
print("  6. LSTM/GRU - Sequential data processing")
print("  7. Embedding - Word/category vectors")
print("  8. Flatten - Conv to Dense transition")

print("\nğŸš€ Next Steps:")
print("  â€¢ Experiment with different architectures")
print("  â€¢ Try Fashion MNIST or CIFAR-10")
print("  â€¢ Use data augmentation")
print("  â€¢ Explore transfer learning")
print("  â€¢ Build custom layers")

print("\nğŸ’¾ Save your model:")
print("  model_complete.save('my_model.h5')")
print("  loaded = keras.models.load_model('my_model.h5')")

print("\n" + "="*80)
print("Happy Deep Learning! ğŸ§ ğŸ¯")
print("="*80)

#works in colab google

