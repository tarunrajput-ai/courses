import React, { useState, useMemo } from 'react';
import './index.css';

// ============================================
// DEMO 1: Token Embedding
// ============================================
function EmbeddingDemo() {
  const [selectedToken, setSelectedToken] = useState(0);
  const vocabulary = { "I": 2, "love": 3, "AI": 4 };
  const embeddingMatrix = {
    2: [0.10, 0.20, 0.30, 0.40],
    3: [0.50, 0.10, 0.80, 0.20],
    4: [0.90, 0.70, 0.20, 0.10],
  };
  const tokens = ["I", "love", "AI"];
  const ids = [2, 3, 4];

  return (
    <div className="p-4 bg-white rounded-lg shadow mb-6">
      <h2 className="text-xl font-bold mb-3 text-purple-700">1. Token Embedding</h2>
      <p className="text-gray-600 mb-3">Convert tokens to vectors via lookup table</p>
      
      <div className="flex gap-2 mb-4">
        {tokens.map((t, i) => (
          <button key={i} onClick={() => setSelectedToken(i)}
            className={`px-3 py-2 rounded ${selectedToken === i ? 'bg-purple-500 text-white' : 'bg-gray-200'}`}>
            "{t}" ‚Üí ID: {ids[i]}
          </button>
        ))}
      </div>
      
      <div className="bg-green-100 p-3 rounded font-mono text-sm">
        <p><strong>Token:</strong> "{tokens[selectedToken]}"</p>
        <p><strong>ID:</strong> {ids[selectedToken]}</p>
        <p><strong>Vector:</strong> [{embeddingMatrix[ids[selectedToken]].join(', ')}]</p>
      </div>
      
      <p className="mt-3 text-sm text-gray-600">
        üí° Embedding is just indexing row {ids[selectedToken]} from the embedding matrix!
      </p>
    </div>
  );
}

// ============================================
// DEMO 2: Positional Encoding
// ============================================
function PositionalEncodingDemo() {
  const [pos, setPos] = useState(1);
  const dModel = 8;

  const calcPE = (p, i) => {
    const div = Math.pow(10000, (2 * Math.floor(i/2)) / dModel);
    return i % 2 === 0 ? Math.sin(p / div) : Math.cos(p / div);
  };

  const peVector = Array(dModel).fill(0).map((_, i) => calcPE(pos, i));

  return (
    <div className="p-4 bg-white rounded-lg shadow mb-6">
      <h2 className="text-xl font-bold mb-3 text-orange-700">2. Positional Encoding</h2>
      <p className="text-gray-600 mb-3">Add position info using sin/cos waves</p>
      
      <div className="mb-4">
        <label className="block mb-1">Position: {pos}</label>
        <input type="range" min="0" max="10" value={pos} 
          onChange={(e) => setPos(parseInt(e.target.value))} className="w-full" />
      </div>
      
      <div className="bg-orange-100 p-3 rounded font-mono text-xs mb-3">
        <p>PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</p>
        <p>PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))</p>
      </div>
      
      <div className="bg-blue-100 p-3 rounded">
        <p className="font-semibold mb-2">PE[{pos}] =</p>
        <div className="flex gap-1 flex-wrap">
          {peVector.map((v, i) => (
            <span key={i} className={`px-2 py-1 rounded text-xs font-mono ${i % 2 === 0 ? 'bg-blue-200' : 'bg-purple-200'}`}>
              {v.toFixed(2)}
            </span>
          ))}
        </div>
        <p className="mt-2 text-xs text-gray-600">
          <span className="bg-blue-200 px-1">sin (even)</span> <span className="bg-purple-200 px-1">cos (odd)</span>
        </p>
      </div>
    </div>
  );
}

// ============================================
// DEMO 3: Q, K, V Projection
// ============================================
function QKVDemo() {
  const X = [[1.34, 0.64, 0.81, 1.20], [1.81, 0.28, 0.22, 1.10], [0.70, 1.70, 0.70, 1.87]];
  const tokens = ["I", "love", "AI"];
  
  // Simplified projection results
  const Q = [[1.10, 0.82, 1.10, 0.89], [1.18, 0.73, 0.98, 0.85], [1.42, 1.12, 1.23, 1.35]];
  const K = [[1.05, 0.95, 0.78, 1.12], [1.02, 0.68, 0.72, 0.92], [1.38, 1.28, 1.05, 1.42]];
  const V = [[0.98, 0.88, 1.02, 0.75], [0.85, 0.72, 0.95, 0.68], [1.25, 1.15, 1.32, 1.08]];

  const fmt = (arr) => `[${arr.map(n => n.toFixed(2)).join(', ')}]`;

  return (
    <div className="p-4 bg-white rounded-lg shadow mb-6">
      <h2 className="text-xl font-bold mb-3 text-blue-700">3. Q, K, V Projections</h2>
      <p className="text-gray-600 mb-3">Q = X¬∑Wq, K = X¬∑Wk, V = X¬∑Wv</p>
      
      <div className="grid grid-cols-3 gap-2 text-xs">
        <div className="bg-blue-100 p-2 rounded">
          <p className="font-bold text-blue-700 mb-1">Query (Q)</p>
          {Q.map((r, i) => <p key={i} className="font-mono">{tokens[i]}: {fmt(r)}</p>)}
          <p className="mt-1 text-gray-600">"What am I looking for?"</p>
        </div>
        <div className="bg-green-100 p-2 rounded">
          <p className="font-bold text-green-700 mb-1">Key (K)</p>
          {K.map((r, i) => <p key={i} className="font-mono">{tokens[i]}: {fmt(r)}</p>)}
          <p className="mt-1 text-gray-600">"What do I contain?"</p>
        </div>
        <div className="bg-purple-100 p-2 rounded">
          <p className="font-bold text-purple-700 mb-1">Value (V)</p>
          {V.map((r, i) => <p key={i} className="font-mono">{tokens[i]}: {fmt(r)}</p>)}
          <p className="mt-1 text-gray-600">"What info do I give?"</p>
        </div>
      </div>
      
      <p className="mt-3 text-sm text-gray-600">
        üí° Weight matrices Wq, Wk, Wv are learned during training. Shape: (d_model √ó d_model)
      </p>
    </div>
  );
}

// ============================================
// DEMO 4: Scaled Dot-Product Attention
// ============================================
function AttentionDemo() {
  const [step, setStep] = useState(0);
  const tokens = ["I", "love", "AI"];
  const d_k = 4;

  const QKt = [[3.79, 3.21, 4.89], [3.63, 3.12, 4.63], [4.88, 4.14, 6.33]];
  const scaled = QKt.map(row => row.map(v => v / 2));
  const softmax = (arr) => {
    const exp = arr.map(x => Math.exp(x));
    const sum = exp.reduce((a, b) => a + b);
    return exp.map(e => e / sum);
  };
  const weights = scaled.map(row => softmax(row));

  const data = [
    { title: "Step 1: QK^T", matrix: QKt, desc: "Dot product similarity" },
    { title: "Step 2: √∑‚àöd_k", matrix: scaled, desc: "Scale by ‚àö4 = 2" },
    { title: "Step 3: Softmax", matrix: weights, desc: "Normalize rows to sum=1" },
  ];

  return (
    <div className="p-4 bg-white rounded-lg shadow mb-6">
      <h2 className="text-xl font-bold mb-3 text-pink-700">4. Scaled Dot-Product Attention</h2>
      <p className="text-gray-600 mb-3">Attention = softmax(QK^T / ‚àöd_k) ¬∑ V</p>
      
      <div className="flex gap-2 mb-4">
        {data.map((d, i) => (
          <button key={i} onClick={() => setStep(i)}
            className={`px-3 py-1 rounded text-sm ${step === i ? 'bg-pink-500 text-white' : 'bg-gray-200'}`}>
            {d.title}
          </button>
        ))}
      </div>
      
      <p className="text-sm mb-2">{data[step].desc}</p>
      
      <table className="text-sm font-mono w-full mb-3">
        <thead>
          <tr className="bg-gray-200">
            <th className="p-2"></th>
            {tokens.map(t => <th key={t} className="p-2">K:{t}</th>)}
          </tr>
        </thead>
        <tbody>
          {data[step].matrix.map((row, i) => (
            <tr key={i} className="border-b">
              <td className="p-2 font-bold">Q:{tokens[i]}</td>
              {row.map((v, j) => (
                <td key={j} className={`p-2 text-center ${step === 2 && v > 0.35 ? 'bg-yellow-200 font-bold' : ''}`}>
                  {v.toFixed(2)}
                </td>
              ))}
            </tr>
          ))}
        </tbody>
      </table>
      
      <p className="text-sm text-gray-600">
        üí° After softmax, each row sums to 1.0 ‚Äî it's a probability distribution over all keys!
      </p>
    </div>
  );
}

// ============================================
// DEMO 5: Multi-Head Attention
// ============================================
function MultiHeadDemo() {
  const [head, setHead] = useState(0);
  const tokens = ["I", "love", "AI"];

  const heads = [
    { name: "Head 1", attn: [[0.45, 0.30, 0.25], [0.35, 0.40, 0.25], [0.30, 0.25, 0.45]], pattern: "Attends to self" },
    { name: "Head 2", attn: [[0.20, 0.50, 0.30], [0.25, 0.25, 0.50], [0.40, 0.35, 0.25]], pattern: "Attends to related words" },
  ];

  return (
    <div className="p-4 bg-white rounded-lg shadow mb-6">
      <h2 className="text-xl font-bold mb-3 text-indigo-700">5. Multi-Head Attention</h2>
      <p className="text-gray-600 mb-3">Multiple heads learn different attention patterns</p>
      
      <div className="flex gap-2 mb-4">
        {heads.map((h, i) => (
          <button key={i} onClick={() => setHead(i)}
            className={`px-3 py-1 rounded ${head === i ? 'bg-indigo-500 text-white' : 'bg-gray-200'}`}>
            {h.name}
          </button>
        ))}
      </div>
      
      <div className="grid grid-cols-2 gap-4">
        <div>
          <p className="font-semibold mb-2">{heads[head].name} Attention Weights</p>
          <table className="text-sm font-mono w-full">
            <thead>
              <tr className="bg-gray-200">
                <th></th>
                {tokens.map(t => <th key={t} className="p-1">{t}</th>)}
              </tr>
            </thead>
            <tbody>
              {heads[head].attn.map((row, i) => (
                <tr key={i}>
                  <td className="p-1 font-bold">{tokens[i]}</td>
                  {row.map((v, j) => (
                    <td key={j} className={`p-1 text-center ${v >= 0.4 ? 'bg-yellow-300 font-bold' : ''}`}>
                      {v.toFixed(2)}
                    </td>
                  ))}
                </tr>
              ))}
            </tbody>
          </table>
        </div>
        <div className="bg-indigo-50 p-3 rounded">
          <p className="font-semibold">Pattern Learned:</p>
          <p className="text-sm">{heads[head].pattern}</p>
          <p className="mt-2 text-xs text-gray-600">
            In real Transformers: 8 heads with d_k = 64 each (512/8)
          </p>
        </div>
      </div>
      
      <div className="mt-4 p-3 bg-purple-100 rounded text-sm">
        <strong>Final:</strong> Concat(head1, head2, ..., head8) ¬∑ W^O ‚Üí back to d_model dimensions
      </div>
    </div>
  );
}

// ============================================
// DEMO 6: Full Transformer Flow
// ============================================
function TransformerFlow() {
  const [active, setActive] = useState(0);

  const steps = [
    { name: "Input Tokens", desc: '"I love AI" ‚Üí [2, 3, 4]', color: "bg-gray-200" },
    { name: "Embedding", desc: "Lookup vectors from matrix", color: "bg-purple-200" },
    { name: "Add Position", desc: "Embedding + PE", color: "bg-orange-200" },
    { name: "Multi-Head Attn", desc: "8 parallel attention heads", color: "bg-blue-200" },
    { name: "Add & Norm", desc: "Residual + LayerNorm", color: "bg-green-200" },
    { name: "Feed Forward", desc: "FFN(x) = ReLU(xW‚ÇÅ)W‚ÇÇ", color: "bg-pink-200" },
    { name: "Add & Norm", desc: "Residual + LayerNorm", color: "bg-green-200" },
    { name: "Output", desc: "Linear ‚Üí Softmax ‚Üí Next token", color: "bg-red-200" },
  ];

  return (
    <div className="p-4 bg-white rounded-lg shadow mb-6">
      <h2 className="text-xl font-bold mb-3 text-gray-700">6. Complete Transformer Flow</h2>
      
      <div className="flex flex-col gap-1">
        {steps.map((s, i) => (
          <div key={i} className="flex items-center gap-2">
            <button onClick={() => setActive(i)}
              className={`flex-1 p-2 rounded text-left text-sm ${active === i ? s.color + ' ring-2 ring-black' : 'bg-gray-100'}`}>
              <span className="font-bold">{i + 1}. {s.name}</span>
              {active === i && <span className="block text-gray-600">{s.desc}</span>}
            </button>
            {i < steps.length - 1 && <span className="text-gray-400">‚Üì</span>}
          </div>
        ))}
      </div>
      
      <p className="mt-4 text-sm text-gray-600">
        üí° Encoder repeats steps 4-7 six times. Decoder is similar but adds masked attention.
      </p>
    </div>
  );
}

// ============================================
// MAIN APP
// ============================================
export default function App() {
  const [activeDemo, setActiveDemo] = useState(0);

  const demos = [
    { name: "All Demos", component: null },
    { name: "1. Embedding", component: EmbeddingDemo },
    { name: "2. Positional", component: PositionalEncodingDemo },
    { name: "3. Q,K,V", component: QKVDemo },
    { name: "4. Attention", component: AttentionDemo },
    { name: "5. Multi-Head", component: MultiHeadDemo },
    { name: "6. Full Flow", component: TransformerFlow },
  ];

  return (
    <div className="min-h-screen bg-gray-100 p-4">
      <div className="max-w-4xl mx-auto">
        <h1 className="text-3xl font-bold text-center mb-2">ü§ñ Transformer: Attention Is All You Need</h1>
        <p className="text-center text-gray-600 mb-6">Interactive Step-by-Step Demos</p>
        
        <div className="flex flex-wrap gap-2 mb-6 justify-center">
          {demos.map((d, i) => (
            <button key={i} onClick={() => setActiveDemo(i)}
              className={`px-3 py-1 rounded text-sm ${activeDemo === i ? 'bg-blue-500 text-white' : 'bg-white'}`}>
              {d.name}
            </button>
          ))}
        </div>

        {activeDemo === 0 ? (
          <>
            <EmbeddingDemo />
            <PositionalEncodingDemo />
            <QKVDemo />
            <AttentionDemo />
            <MultiHeadDemo />
            <TransformerFlow />
          </>
        ) : (
          <>{React.createElement(demos[activeDemo].component)}</>
        )}
      </div>
    </div>
  );
}
